# LLM Evaluation Report

This document presents a structured evaluation of one or more Large Language Models (LLMs)
using a combination of classical NLP metrics, RAG-specific checks, and LLM-as-a-Judge scoring.

The report is designed for **engineering review, CI/CD quality gating, and model comparison**
in production environments.

---

## Run Metadata

- **Framework Version:** {{ results.metadata.version }}
- **Benchmark Dataset:** {{ results.metadata.dataset }}
- **Total Evaluation Samples:** {{ results.metadata.num_examples }}
- **Execution Timestamp:** {{ results.metadata.timestamp }}

---

## Model Performance Summary

The tables below show **aggregate statistics** computed across the full benchmark dataset.
All scores are normalized to the range **[0.0 â€“ 1.0]**, where higher values indicate better performance.

{% for model, metrics in results.aggregates.items() %}
### ðŸ”¹ Model: **{{ model }}**

| Metric | Mean | Median | Std Dev | Min | Max |
|-------|------|--------|---------|-----|-----|
{% for metric, stats in metrics.items() %}
| {{ metric }} | {{ "%.3f"|format(stats.mean) }} | {{ "%.3f"|format(stats.median) }} | {{ "%.3f"|format(stats.std) }} | {{ "%.3f"|format(stats.min) }} | {{ "%.3f"|format(stats.max) }} |
{% endfor %}

{% endfor %}

---

## Quality Gate Results

Quality gates are enforced to prevent regressions and ensure minimum acceptable performance
before models are promoted to downstream environments.

{% if results.quality_gates.passed %}
**All quality gates passed.**  
The evaluated model(s) satisfy all required thresholds.
{% else %}
**One or more quality gates failed.**  
This evaluation run requires review before deployment.
{% endif %}

| Metric | Threshold | Observed | Status |
|-------|-----------|----------|--------|
{% for gate in results.quality_gates.details %}
| {{ gate.metric }} | {{ gate.threshold }} | {{ "%.3f"|format(gate.actual) }} | {{ gate.status }} |
{% endfor %}

---

## Interpretation & Engineering Insights

- **Faithfulness** indicates whether generated answers are grounded in retrieved context and helps detect hallucinations.
- **Context Relevancy** reflects retrieval quality and alignment between the query and retrieved documents.
- **Answer Relevancy** measures how directly and accurately the response addresses the original query.
- **LLM-as-a-Judge** scores provide a higher-level qualitative assessment across coherence, relevance, and safety.

This report can be used to:
- Compare multiple model variants or prompts
- Track performance trends over time
- Enforce automated CI/CD quality gates
- Support data-driven model selection decisions

---

*Generated by **llm-eval** â€” a production-grade LLM evaluation framework built for reliability,
extensibility, and real-world deployment.*
