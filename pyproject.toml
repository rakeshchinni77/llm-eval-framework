[tool.poetry]
name = "llm-eval"
version = "0.1.0"
description = "Production-grade LLM evaluation framework with multi-metric analysis"
authors = ["Chinni Rakesh <rakeshchinni0000@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [{ include = "llm_eval", from = "src" }]

[tool.poetry.dependencies]
python = "^3.9"

# CLI
typer = "^0.12.0"
rich = "^13.7.0"

# Config & Validation
pydantic = "^2.6.0"
pyyaml = "^6.0.1"

# Data
pandas = "^2.2.0"
numpy = "^1.26.0"

# NLP & Metrics
nltk = "^3.8.1"
evaluate = "^0.4.1"
sentence-transformers = "^2.5.1"
transformers = "^4.39.0"

# LLM Providers
openai = "^1.14.0"
anthropic = "^0.25.0"

# Retry & Resilience
tenacity = "^8.2.3"

# Reporting
jinja2 = "^3.1.3"

# Visualization
matplotlib = "^3.8.3"
seaborn = "^0.13.2"

[tool.poetry.group.dev.dependencies]
pytest = "^8.1.1"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"

[tool.poetry.scripts]
llm-eval = "llm_eval.cli.main:app"

[build-system]
requires = ["poetry-core>=1.8.0"]
build-backend = "poetry.core.masonry.api"
